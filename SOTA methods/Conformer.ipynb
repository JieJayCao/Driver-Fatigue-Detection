{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torch import Tensor\n",
    "import math\n",
    "from thop import profile\n",
    "from thop import clever_format\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "config = {\n",
    "    'subjects_num': 12,\n",
    "    'n_epochs': 100, \n",
    "    'batch_size': 64,\n",
    "    'save_name': 'logs/Conformer-{epoch:02d}-{val_acc:.2f}',\n",
    "    'log_path1': 'logs/Conformer_logs',\n",
    "    'num_class': 2\n",
    "}\n",
    "\n",
    "isIntraSub = False\n",
    "\n",
    "def get_device():\n",
    "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class EEG_IntraSub_Dataset(Dataset):\n",
    "    def __init__(self, path, mode, test_sub):\n",
    "        self.mode = mode\n",
    "        sub_list = [i for i in range(config['subjects_num'])]\n",
    "        data = []\n",
    "        label = []\n",
    "        \n",
    "        for i in sub_list:\n",
    "            data_sub = np.load(path + f'sub_{i}_eeg.npy')\n",
    "            label_sub = np.load(path + f'sub_{i}_labels.npy')\n",
    "            data.extend(data_sub)\n",
    "            label.extend(label_sub)\n",
    "            \n",
    "        data = np.array(data)\n",
    "        label = np.array(label).flatten()\n",
    "        \n",
    "        shuffle_idx = np.random.permutation(len(data))\n",
    "        data = data[shuffle_idx]\n",
    "        label = label[shuffle_idx]\n",
    "    \n",
    "        if mode == 'train':\n",
    "            data = data[:int(len(data)*0.8)]\n",
    "            label = label[:int(len(label)*0.8)]\n",
    "       \n",
    "        elif mode == 'val':\n",
    "            data = data[int(len(data)*0.8):int(len(data)*0.9)]\n",
    "            label = label[int(len(label)*0.8):int(len(label)*0.9)]\n",
    "        \n",
    "        elif mode == 'test':\n",
    "            data = data[int(len(data)*0.9):]\n",
    "            label = label[int(len(label)*0.9):]\n",
    "        \n",
    "        self.data = torch.FloatTensor(data).unsqueeze(1)\n",
    "        self.label = torch.LongTensor(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.label[index]\n",
    "        \n",
    "class EEG_InterSub_Dataset(Dataset):\n",
    "    def __init__(self, path, mode, test_sub):\n",
    "        self.mode = mode\n",
    "        self.test_sub = test_sub\n",
    "        \n",
    "        if mode == 'train' or mode == 'val':\n",
    "            train_sub = [i for i in range(config['subjects_num'])]\n",
    "            train_sub.remove(test_sub)\n",
    "            data = []\n",
    "            label = []\n",
    "            for i in train_sub:\n",
    "                data_sub = np.load(path + f'sub_{i}_eeg.npy')\n",
    "                label_sub = np.load(path + f'sub_{i}_labels.npy')\n",
    "                data.extend(data_sub)\n",
    "                label.extend(label_sub)\n",
    "                \n",
    "            data = np.array(data)\n",
    "            label = np.array(label).flatten()\n",
    "\n",
    "            shuffle_idx = np.random.permutation(len(data))\n",
    "            data = data[shuffle_idx]\n",
    "            label = label[shuffle_idx]\n",
    "    \n",
    "            if mode == 'train':\n",
    "                data = data[:int(len(data)*0.8)]\n",
    "                label = label[:int(len(label)*0.8)]\n",
    "                \n",
    "            elif mode == 'val':\n",
    "                data = data[int(len(data)*0.8):]\n",
    "                label = label[int(len(label)*0.8):]\n",
    "                   \n",
    "        elif mode == 'test':\n",
    "            data = np.load(path + f'sub_{test_sub}_eeg.npy')\n",
    "            label = np.load(path + f'sub_{test_sub}_labels.npy')\n",
    "\n",
    "        self.data = torch.FloatTensor(data).unsqueeze(1)\n",
    "        self.label = torch.LongTensor(label)      \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.label[index]\n",
    "\n",
    "def prep_dataloader(path, mode, batch_size, test_sub, isIntraSub = False, njobs=1):\n",
    "    if isIntraSub:\n",
    "        print(\"IntraSub\")\n",
    "        dataset = EEG_IntraSub_Dataset(path, mode, test_sub)\n",
    "    else:\n",
    "        print(\"InterSub\")\n",
    "        dataset = EEG_InterSub_Dataset(path, mode, test_sub)\n",
    "        \n",
    "    dataloader = DataLoader(dataset, batch_size, shuffle=(mode == 'train'), drop_last=False, num_workers=njobs,\n",
    "                            pin_memory=True)\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, emb_size=40):\n",
    "        # self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "\n",
    "        self.shallownet = nn.Sequential(\n",
    "            nn.Conv2d(1, 40, (1, 25), (1, 1)),\n",
    "            nn.Conv2d(40, 40, (17, 1), (1, 1)),\n",
    "            nn.BatchNorm2d(40),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 75), (1, 15)),  # pooling acts as slicing to obtain 'patch' along the time dimension as in ViT\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(40, emb_size, (1, 1), stride=(1, 1)),  # transpose, conv could enhance fiting ability slightly\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.shallownet(x)\n",
    "        x = self.projection(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        self.keys = nn.Linear(emb_size, emb_size)\n",
    "        self.queries = nn.Linear(emb_size, emb_size)\n",
    "        self.values = nn.Linear(emb_size, emb_size)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
    "        queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        values = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)  \n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "\n",
    "        scaling = self.emb_size ** (1 / 2)\n",
    "        att = F.softmax(energy / scaling, dim=-1)\n",
    "        att = self.att_drop(att)\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size, expansion, drop_p):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return input*0.5*(1.0+torch.erf(input/math.sqrt(2.0)))\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size,\n",
    "                 num_heads=10,\n",
    "                 drop_p=0.5,\n",
    "                 forward_expansion=4,\n",
    "                 forward_drop_p=0.5):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size, num_heads, drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )\n",
    "            ))\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth, emb_size):\n",
    "        super().__init__(*[TransformerEncoderBlock(emb_size) for _ in range(depth)])\n",
    "\n",
    "\n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, emb_size, n_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # global average pooling\n",
    "        self.clshead = nn.Sequential(\n",
    "            Reduce('b n e -> b e', reduction='mean'),\n",
    "            nn.LayerNorm(emb_size),\n",
    "            nn.Linear(emb_size, n_classes)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(800, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, config['num_class'])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        out = self.fc(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Conformer(pl.LightningModule):\n",
    "    def __init__(self, emb_size=40, depth=6, n_classes=config['num_class'], **kwargs):\n",
    "        super(Conformer, self).__init__()\n",
    "\n",
    "        self.patch_embedding = PatchEmbedding(emb_size)\n",
    "        self.transformer = TransformerEncoder(depth, emb_size)\n",
    "        self.classifier = ClassificationHead(emb_size, n_classes)\n",
    "        self._init_weights()\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    def _init_weights(self):\n",
    "        # 对卷积层使用He初始化\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            # 对全连接层使用Xavier初始化    \n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            # 对BatchNorm层使用常数初始化\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        # Patch嵌入\n",
    "        x = self.patch_embedding(x)\n",
    "        \n",
    "        # Transformer编码\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # 分类头\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        #optimizer = torch.optim.AdamW(self.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        \"\"\"scheduler = {\n",
    "            'scheduler': ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True),\n",
    "            'monitor': 'val_loss',  # 根据验证集损失调整学习率\n",
    "        }\"\"\"\n",
    "        return optimizer #, [scheduler]\n",
    "      \n",
    "    def training_step(self, batch):\n",
    "        x, y = batch\n",
    "        preds = self(x)\n",
    "        loss = F.cross_entropy(preds, y)\n",
    "        self.log('training_loss', loss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
    "        loss = {'loss': loss}\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        x, y = batch\n",
    "        preds = self(x)\n",
    "        loss = F.cross_entropy(preds, y)\n",
    "        self.log('val_loss', loss, prog_bar=True, logger=True, on_step=False, on_epoch=True)\n",
    "\n",
    "    def test_step(self, batch):\n",
    "        x, y = batch\n",
    "        preds = self(x)\n",
    "        \n",
    "        y_pre = torch.argmax(F.log_softmax(preds, dim=1), dim=1)\n",
    "        acc = accuracy_score(y.cpu(), y_pre.cpu())\n",
    "        pre = precision_score(y.cpu(), y_pre.cpu(), average='weighted')\n",
    "        recall = recall_score(y.cpu(), y_pre.cpu(), average='weighted')\n",
    "        f1 = f1_score(y.cpu(), y_pre.cpu(), average='weighted')\n",
    "\n",
    "        self.log('test_acc', acc)\n",
    "        self.log('test_pre', pre)\n",
    "        self.log('test_recall', recall)\n",
    "        self.log('test_f1', f1)\n",
    "        \n",
    "           \n",
    "        return {'test_acc': acc, 'test_pre': pre, 'test_recall': recall, 'test_f1': f1} \n",
    "\n",
    "\n",
    "def predict(model, dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            x, y = batch\n",
    "            preds = model(x)\n",
    "            y_pre = torch.argmax(F.log_softmax(preds, dim=1), dim=1)\n",
    "            acc = accuracy_score(y.cpu(), y_pre.cpu())\n",
    "            pre = precision_score(y.cpu(), y_pre.cpu(), average='weighted')\n",
    "            recall = recall_score(y.cpu(), y_pre.cpu(), average='weighted')\n",
    "            f1 = f1_score(y.cpu(), y_pre.cpu(), average='weighted')\n",
    "\n",
    "    return acc, pre, recall, f1\n",
    "       \n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    filename=config['save_name'],\n",
    "    save_top_k=1,\n",
    "    mode='min',\n",
    "    save_last=True\n",
    ")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tr_path = val_path = test_path =  \"/home/jie/Program/872/Dataset/SEED-VIG-Subset/\"\n",
    "    device = get_device()\n",
    "    isIntraSub=False\n",
    "    \n",
    "    # 创建模型并计算FLOPs\n",
    "    model = Conformer()\n",
    "    input = torch.randn(1, 1, 17, 384)\n",
    "    flops, params = profile(model, inputs=(input,))\n",
    "    flops, params = clever_format([flops, params], \"%.3f\")\n",
    "    print(f\"FLOPs: {flops}, Parameters: {params}\")\n",
    "    \n",
    "    AC,PR,RE,F1 = 0,0,0,0\n",
    "    for test_sub in range(config['subjects_num']):\n",
    "        tr_set = prep_dataloader(tr_path, 'train', config['batch_size'], test_sub, isIntraSub, njobs=6)\n",
    "        val_set = prep_dataloader(val_path, 'val', config['batch_size'], test_sub, isIntraSub, njobs=6)\n",
    "        test_set = prep_dataloader(test_path, 'test', config['batch_size'], test_sub, isIntraSub, njobs=1)\n",
    "        model =  Conformer().to(device)\n",
    "        logger = TensorBoardLogger(config['log_path1'])#, config['log_path2'])\n",
    "        trainer = Trainer(val_check_interval=1.0, max_epochs=config['n_epochs'], devices=[0], accelerator='gpu',\n",
    "                        logger=logger,\n",
    "                        callbacks=[\n",
    "                            #EarlyStopping(monitor='val_loss', mode='min', check_on_train_epoch_end=True, patience=10, min_delta=1e-4),\n",
    "                            checkpoint_callback\n",
    "                        ]\n",
    "                        )\n",
    "        \n",
    "        trainer.fit(model, train_dataloaders=tr_set, val_dataloaders=val_set)\n",
    "        # 保存最终模型\n",
    "        #trainer.save_checkpoint('FastAlertNet_final.ckpt')\n",
    "        test_results = trainer.test(model, dataloaders=test_set)\n",
    "       \n",
    "        AC += test_results[0]['test_acc']\n",
    "        PR += test_results[0]['test_pre']\n",
    "        RE += test_results[0]['test_recall']\n",
    "        F1 += test_results[0]['test_f1']\n",
    "        \n",
    "    AC /= config['subjects_num']\n",
    "    PR /= config['subjects_num'] \n",
    "    RE /= config['subjects_num']\n",
    "    F1 /= config['subjects_num']\n",
    "    print(f\"&{AC*100:.2f}\",f\"&{PR*100:.2f}\",f\"&{RE*100:.2f}\",f\"&{F1*100:.2f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Conformer()\n",
    "out = model(torch.randn(1, 1, 17, 384))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
